{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning Deep Gaussian Process Regression Monte Carlo Simulation\n",
    "\n",
    "https://docs.gpytorch.ai/en/latest/examples/01_Exact_GPs/Simple_GP_Regression.html\n",
    "\n",
    "https://docs.gpytorch.ai/en/stable/examples/06_PyTorch_NN_Integration_DKL/KISSGP_Deep_Kernel_Regression_CUDA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.special import ndtri\n",
    "import random\n",
    "\n",
    "from core.SNDGPR.train import train_model\n",
    "from core.MCS import MC_sampling_plan, MC_prediction, estimate_Pf\n",
    "from core.bay_opt.K_fold_train import kfold_train\n",
    "from core.bay_opt.bayesian_optimization import bayesian_optimization\n",
    "from core.bay_opt.optimization_variables import optimization_variables\n",
    "\n",
    "from core.utils import load_core_modules, load_example_modules, sample_info, print_info, \\\n",
    "    save_bests, plot_losses, min_max_normalization, save_x_added, evaluate_g, \\\n",
    "    pickle_save, pickle_load, results_plot, results_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# For NumPy\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# For PyTorch\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)  # if you are using multi-GPU\n",
    "\n",
    "# For Python's built-in random module\n",
    "random.seed(SEED)\n",
    "\n",
    "# Ensuring reproducibility in cuDNN using PyTorch\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_MC = 1e6  # number of points for Monte Carlo\n",
    "TRAINING_ITERATIONS = 1000  # epochs for training the DGPR\n",
    "LEARNING_RATE = 0.01  # learning rate for training the model with Adam\n",
    "VALIDATION_SPLIT = 6  # K-Fold split (K = VALIDATION_SPLIT)\n",
    "N = 48  # initial number of points\n",
    "N_INFILL = 12  # number of infill points\n",
    "ALPHA = 0.05  # alpha for confidence bounds\n",
    "SPECTRAL_NORMALIZATION = True\n",
    "EXAMPLE = 'collapse_simulation'  # example to run\n",
    "SAMPLING_PLAN_STRATEGY = 'LHS'  # initial sampling plan generation strategy\n",
    "LEARNING_FUNCTION = 'U'  # learning function\n",
    "CONVERGENCE_FUNCTION = 'stop_Pf'  # stopping criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters of Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_INITIAL_EGO = 25\n",
    "N_INFILL_EGO = 15\n",
    "\n",
    "DIM_EGO = 3  # number of hyperparameters to optimize\n",
    "TRAINING_ITERATIONS_EGO = 10000\n",
    "BOUNDS_BSA = (\n",
    "    tuple((0, 1) for _ in range(DIM_EGO))  # the number of (0, 1) tuples has to be equal to DIM\n",
    "    )\n",
    "BSA_POPSIZE = 20\n",
    "BSA_EPOCH = 200\n",
    "# Adjust BSA population size and epochs based on the size of your search space,\n",
    "# defined by BOUNDS_BAY_OPT.\n",
    "# Make BSA_POPSIZE * BSA_EPOCH much larger than your search space.\n",
    "BOUNDS_BAY_OPT = [[1, 5], [2, 10], [0, 4]]  # L, r, act_fun\n",
    "# For more activation functions, check optimization_variables.py\n",
    "# and adjust the nonlinearity of the weight initialization in SNDGPR.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load random variables and limit state function dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RVs, limit_state_function = load_example_modules(EXAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load core functions dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_sampling_plan, learning_function, evaluate_lf, convergence_function = load_core_modules(\n",
    "    SAMPLING_PLAN_STRATEGY, LEARNING_FUNCTION, CONVERGENCE_FUNCTION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Params = dict()\n",
    "Params['RVs'] = RVs\n",
    "Params['N_MC'] = N_MC\n",
    "Params['training_iterations'] = TRAINING_ITERATIONS\n",
    "Params['N'] = N\n",
    "Params['N_added'] = N_INFILL\n",
    "Params['alpha'] = ALPHA\n",
    "Params['limit_state_function'] = EXAMPLE\n",
    "Params['initial_sampling_plan'] = SAMPLING_PLAN_STRATEGY\n",
    "Params['learning_function'] = LEARNING_FUNCTION\n",
    "Params['convergence_function'] = CONVERGENCE_FUNCTION\n",
    "Params['seed'] = SEED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = initial_sampling_plan(N, RVs, SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_candidate = MC_sampling_plan(N_MC, RVs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_info(RVs, [SAMPLING_PLAN_STRATEGY, 'Monte Carlo'], [x, x_candidate])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = x.shape[1]\n",
    "estimate_Pf_all = []\n",
    "estimate_Pf_allp = []\n",
    "estimate_Pf_allm = []\n",
    "estimate_N_samples_added = []\n",
    "N_samples_added_total = 0\n",
    "converged = False\n",
    "it = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    print(f'\\nIteration {it}')\n",
    "    \n",
    "    if it == 0:\n",
    "        f_EGO, x_EGO, model, likelihood, train_losses, val_losses, train_x, val_x, train_g, val_g, x_max, x_min \\\n",
    "            = bayesian_optimization(\n",
    "                x, N_INITIAL_EGO, N_INFILL_EGO, DIM_EGO, TRAINING_ITERATIONS_EGO, BOUNDS_BSA, BSA_POPSIZE, BSA_EPOCH, SEED, \\\n",
    "                TRAINING_ITERATIONS, BOUNDS_BAY_OPT, SPECTRAL_NORMALIZATION\n",
    "                )\n",
    "    else:\n",
    "        layer_sizes, act_fun = optimization_variables(BOUNDS_BAY_OPT, x_EGO[torch.argmin(f_EGO), :], x, SPECTRAL_NORMALIZATION)\n",
    "        \n",
    "        _, _, model, likelihood, train_losses, val_losses, train_x, val_x, train_g, val_g, x_max, x_min, fold = kfold_train(\n",
    "            x, g, x_candidate, TRAINING_ITERATIONS, LEARNING_RATE, layer_sizes, act_fun, train_model, MC_prediction, evaluate_lf, estimate_Pf, learning_function,\n",
    "                N, N_MC, ALPHA, SPECTRAL_NORMALIZATION, n_splits=VALIDATION_SPLIT, SEED=SEED\n",
    "            )\n",
    "    \n",
    "    # Save variables and plot loss\n",
    "    save_bests(model, likelihood, train_losses, val_losses, x, train_x, val_x, train_g, val_g,\n",
    "               x_EGO, f_EGO, x_max, x_min, it, BOUNDS_BAY_OPT, SPECTRAL_NORMALIZATION, EXAMPLE)\n",
    "    plot_losses(train_losses, val_losses, it)\n",
    "    \n",
    "    # Predict MC responses (only the sample which are not contained in the Kriging yet)\n",
    "    x_candidate_normalized = min_max_normalization(x_max, x_min, x_candidate)\n",
    "    preds = MC_prediction(model, likelihood, x_candidate_normalized)\n",
    "    \n",
    "    # Evaluate learning function\n",
    "    g_mean, gs, ind_lf = evaluate_lf(preds, learning_function)\n",
    "    \n",
    "    # Select additional sample (the sample which maximizes the learning function value)\n",
    "    x_added = x_candidate[ind_lf, :]\n",
    "    x_added = x_added.view(1, data_dim)\n",
    "    # x_added = x_added * (x_max - x_min) + x_min  # undo normalization\n",
    "    \n",
    "    save_x_added(x_added, it, EXAMPLE)  # Salve o array do NumPy em um arquivo .mat\n",
    "    \n",
    "    # Estimate Pf\n",
    "    Pf, Pf_plus, Pf_minus = estimate_Pf(g, g_mean, gs, N, N_MC, ALPHA)\n",
    "    \n",
    "    estimate_Pf_all.append(Pf)\n",
    "    estimate_Pf_allp.append(Pf_plus)\n",
    "    estimate_Pf_allm.append(Pf_minus)\n",
    "    estimate_N_samples_added.append(N_samples_added_total)\n",
    "    \n",
    "    # Print some info\n",
    "    print_info(N, N_INFILL, it, Pf, Pf_plus, Pf_minus)\n",
    "    \n",
    "    # Check if maximum number of points were added\n",
    "    if N_samples_added_total >= N_INFILL: break\n",
    "    it += 1\n",
    "    \n",
    "    # Convergence criterion\n",
    "    if converged and N_samples_added_total != 0:\n",
    "        if convergence_function(g, g_mean, gs, N, N_MC): break\n",
    "        converged = False\n",
    "    else:\n",
    "        converged = convergence_function(g, g_mean, gs, N, N_MC)\n",
    "    \n",
    "    g_added = evaluate_g(x_added, it, limit_state_function, EXAMPLE)\n",
    "    \n",
    "    x = torch.cat((x, x_added), 0)\n",
    "    g = torch.cat((g, g_added), 0)\n",
    "    x_candidate = torch.cat((x_candidate[:ind_lf], x_candidate[ind_lf+1:]))\n",
    "    N_samples_added_total = N_samples_added_total + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "# Estimate failure probability\n",
    "estimate_Pf_0 = (torch.sum(g_mean <= 0) + torch.sum(g[N+1:] <= 0))/N_MC\n",
    "\n",
    "# Estimate the covariance\n",
    "estimate_CoV = torch.sqrt((1-estimate_Pf_0) / estimate_Pf_0 / N_MC)\n",
    "\n",
    "# Store the results\n",
    "Results = {\n",
    "    'Pf': estimate_Pf_0,\n",
    "    'Beta': -ndtri(estimate_Pf_0),\n",
    "    'CoV': estimate_CoV,\n",
    "    'Model_Evaluations': N_samples_added_total + N,\n",
    "    'Pf_CI': estimate_Pf_0 * np.array([\n",
    "        1 + ndtri(ALPHA/2)*estimate_CoV,\n",
    "        1 + ndtri(1-ALPHA/2)*estimate_CoV\n",
    "        ]),\n",
    "    }\n",
    "Results['Beta_CI'] = torch.flip(-ndtri(Results['Pf_CI']), [0])\n",
    "\n",
    "History = {\n",
    "    'Pf': estimate_Pf_all,\n",
    "    'Pf_Upper': estimate_Pf_allp,\n",
    "    'Pf_Lower': estimate_Pf_allm,\n",
    "    'N_Samples': estimate_N_samples_added,\n",
    "    'N_Init': N,\n",
    "    'X': x,\n",
    "    'G': g,\n",
    "    'MC_Sample': x_candidate,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_save(Results, History, Params, EXAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display results and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_print(Results, History, Params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_plot(Results, History, Params, EXAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results, History, Params = pickle_load(EXAMPLE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
