{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning Deep Gaussian Process Regression Monte Carlo Simulation\n",
    "\n",
    "https://docs.gpytorch.ai/en/latest/examples/01_Exact_GPs/Simple_GP_Regression.html\n",
    "\n",
    "https://docs.gpytorch.ai/en/stable/examples/06_PyTorch_NN_Integration_DKL/KISSGP_Deep_Kernel_Regression_CUDA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.special import ndtri\n",
    "import random\n",
    "\n",
    "from core.SNDGPR.train import train_model\n",
    "from core.MCS import MC_sampling_plan, MC_prediction, estimate_Pf\n",
    "from core.bay_opt.K_fold_train import kfold_train\n",
    "from core.bay_opt.bayesian_optimization import bayesian_optimization\n",
    "from core.bay_opt.optimization_variables import optimization_variables\n",
    "\n",
    "from core.utils import load_core_modules, load_example_modules, sample_info, print_info, \\\n",
    "    save_bests, plot_losses, min_max_normalization, save_x_added, evaluate_g, \\\n",
    "    pickle_save, pickle_load, results_plot, results_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# For NumPy\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# For PyTorch\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)  # if you are using multi-GPU\n",
    "\n",
    "# For Python's built-in random module\n",
    "random.seed(SEED)\n",
    "\n",
    "# Ensuring reproducibility in cuDNN using PyTorch\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_MC = 1e6  # number of points for Monte Carlo\n",
    "TRAINING_ITERATIONS = 1000  # epochs for training the DGPR\n",
    "LEARNING_RATE = 0.01  # learning rate for training the model with Adam\n",
    "VALIDATION_SPLIT = 6  # K-Fold split (K = VALIDATION_SPLIT)\n",
    "N = 81  # initial number of points\n",
    "N_INFILL = 500  # number of infill points\n",
    "ALPHA = 0.05  # alpha for confidence bounds\n",
    "SPECTRAL_NORMALIZATION = True\n",
    "EXAMPLE = 'example1'  # example to run\n",
    "SAMPLING_PLAN_STRATEGY = 'LHS'  # initial sampling plan generation strategy\n",
    "LEARNING_FUNCTION = 'U'  # learning function\n",
    "CONVERGENCE_FUNCTION = 'stop_Pf'  # stopping criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters of Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_INITIAL_EGO = 25\n",
    "N_INFILL_EGO = 15\n",
    "\n",
    "DIM_EGO = 3  # number of hyperparameters to optimize\n",
    "TRAINING_ITERATIONS_EGO = 10000\n",
    "BOUNDS_BSA = (\n",
    "    tuple((0, 1) for _ in range(DIM_EGO))  # the number of (0, 1) tuples has to be equal to DIM\n",
    "    )\n",
    "BSA_POPSIZE = 20\n",
    "BSA_EPOCH = 200\n",
    "# Adjust BSA population size and epochs based on the size of your search space,\n",
    "# defined by BOUNDS_BAY_OPT.\n",
    "# Make BSA_POPSIZE * BSA_EPOCH much larger than your search space.\n",
    "BOUNDS_BAY_OPT = [[1, 5], [1, 5], [0, 4]]  # L, r, act_fun\n",
    "# For more activation functions, check optimization_variables.py\n",
    "# and adjust the nonlinearity of the weight initialization in SNDGPR.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load random variables and limit state function dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RVs, limit_state_function = load_example_modules(EXAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load core functions dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_sampling_plan, learning_function, evaluate_lf, convergence_function = load_core_modules(\n",
    "    SAMPLING_PLAN_STRATEGY, LEARNING_FUNCTION, CONVERGENCE_FUNCTION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Params = dict()\n",
    "Params['RVs'] = RVs\n",
    "Params['N_MC'] = N_MC\n",
    "Params['training_iterations'] = TRAINING_ITERATIONS\n",
    "Params['N'] = N\n",
    "Params['N_added'] = N_INFILL\n",
    "Params['alpha'] = ALPHA\n",
    "Params['limit_state_function'] = EXAMPLE\n",
    "Params['initial_sampling_plan'] = SAMPLING_PLAN_STRATEGY\n",
    "Params['learning_function'] = LEARNING_FUNCTION\n",
    "Params['convergence_function'] = CONVERGENCE_FUNCTION\n",
    "Params['seed'] = SEED\n",
    "Params['SN'] = SPECTRAL_NORMALIZATION\n",
    "\n",
    "Params['N_EGO'] = N_INITIAL_EGO\n",
    "Params['N_INFILL_EGO'] = N_INFILL_EGO\n",
    "Params['DIM_EGO'] = DIM_EGO\n",
    "Params['TRAINING_ITERATIONS_EGO'] = TRAINING_ITERATIONS_EGO\n",
    "Params['BSA_POPSIZE'] = BSA_POPSIZE\n",
    "Params['BSA_EPOCHS'] = BSA_EPOCH\n",
    "Params['BOUNDS_BAY_OPT'] = BOUNDS_BAY_OPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = initial_sampling_plan(N, RVs, SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_candidate = MC_sampling_plan(N_MC, RVs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RV: x_1 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.28, min: 0.77\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.46, min: 0.51\n",
      "\n",
      "RV: x_2 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.33, min: 0.74\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.48, min: 0.51\n",
      "\n",
      "RV: x_3 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.24, min: 0.74\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.46, min: 0.52\n",
      "\n",
      "RV: x_4 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.27, min: 0.72\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.51, min: 0.51\n",
      "\n",
      "RV: x_5 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.32, min: 0.67\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.45, min: 0.51\n",
      "\n",
      "RV: x_6 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.25, min: 0.76\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.52, min: 0.52\n",
      "\n",
      "RV: x_7 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.35, min: 0.75\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.48, min: 0.50\n",
      "\n",
      "RV: x_8 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.24, min: 0.75\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.50, min: 0.51\n",
      "\n",
      "RV: x_9 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.27, min: 0.77\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.50, min: 0.52\n",
      "\n",
      "RV: x_10 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.25, min: 0.65\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.49, min: 0.51\n",
      "\n",
      "RV: x_11 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.29, min: 0.67\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.48, min: 0.54\n",
      "\n",
      "RV: x_12 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.24, min: 0.71\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.50, min: 0.51\n",
      "\n",
      "RV: x_13 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.24, min: 0.75\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.47, min: 0.52\n",
      "\n",
      "RV: x_14 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.28, min: 0.75\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.51, min: 0.50\n",
      "\n",
      "RV: x_15 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.23, min: 0.75\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.50, min: 0.51\n",
      "\n",
      "RV: x_16 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.23, min: 0.77\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.47, min: 0.53\n",
      "\n",
      "RV: x_17 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.23, min: 0.76\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.50, min: 0.49\n",
      "\n",
      "RV: x_18 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.23, min: 0.77\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.50, min: 0.56\n",
      "\n",
      "RV: x_19 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.25, min: 0.76\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.46, min: 0.51\n",
      "\n",
      "RV: x_20 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.33, min: 0.77\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.49, min: 0.52\n",
      "\n",
      "RV: x_21 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.29, min: 0.75\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.46, min: 0.50\n",
      "\n",
      "RV: x_22 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.28, min: 0.77\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.51, min: 0.51\n",
      "\n",
      "RV: x_23 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.30, min: 0.71\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.47, min: 0.45\n",
      "\n",
      "RV: x_24 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.24, min: 0.76\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.46, min: 0.52\n",
      "\n",
      "RV: x_25 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.29, min: 0.76\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.47, min: 0.50\n",
      "\n",
      "RV: x_26 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.23, min: 0.77\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.48, min: 0.52\n",
      "\n",
      "RV: x_27 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.24, min: 0.76\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.45, min: 0.54\n",
      "\n",
      "RV: x_28 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.23, min: 0.68\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.48, min: 0.50\n",
      "\n",
      "RV: x_29 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.23, min: 0.75\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.48, min: 0.49\n",
      "\n",
      "RV: x_30 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.23, min: 0.77\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.45, min: 0.52\n",
      "\n",
      "RV: x_31 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.28, min: 0.77\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.50, min: 0.52\n",
      "\n",
      "RV: x_32 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.23, min: 0.77\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.53, min: 0.44\n",
      "\n",
      "RV: x_33 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.24, min: 0.76\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.47, min: 0.53\n",
      "\n",
      "RV: x_34 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.23, min: 0.71\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.49, min: 0.53\n",
      "\n",
      "RV: x_35 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.33, min: 0.77\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.47, min: 0.51\n",
      "\n",
      "RV: x_36 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.28, min: 0.72\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.52, min: 0.51\n",
      "\n",
      "RV: x_37 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.25, min: 0.75\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.49, min: 0.51\n",
      "\n",
      "RV: x_38 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.27, min: 0.77\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.49, min: 0.48\n",
      "\n",
      "RV: x_39 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.23, min: 0.74\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.49, min: 0.53\n",
      "\n",
      "RV: x_40 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.25, min: 0.74\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.49, min: 0.47\n",
      "\n",
      "RV: x_41 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.22, min: 0.74\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.48, min: 0.48\n",
      "\n",
      "RV: x_42 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.24, min: 0.75\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.49, min: 0.55\n",
      "\n",
      "RV: x_43 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.23, min: 0.77\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.47, min: 0.54\n",
      "\n",
      "RV: x_44 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.28, min: 0.73\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.49, min: 0.49\n",
      "\n",
      "RV: x_45 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.23, min: 0.76\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.58, min: 0.49\n",
      "\n",
      "RV: x_46 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.27, min: 0.71\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.47, min: 0.56\n",
      "\n",
      "RV: x_47 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.23, min: 0.77\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.52, min: 0.54\n",
      "\n",
      "RV: x_48 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.32, min: 0.77\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.48, min: 0.50\n",
      "\n",
      "RV: x_49 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.23, min: 0.76\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.49, min: 0.55\n",
      "\n",
      "RV: x_50 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.24, min: 0.77\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.49, min: 0.54\n",
      "\n",
      "RV: x_51 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.23, min: 0.68\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.52, min: 0.52\n",
      "\n",
      "RV: x_52 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.25, min: 0.77\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.48, min: 0.48\n",
      "\n",
      "RV: x_53 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.28, min: 0.75\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.51, min: 0.50\n",
      "\n",
      "RV: x_54 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.27, min: 0.75\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.47, min: 0.53\n",
      "\n",
      "RV: x_55 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.26, min: 0.77\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.50, min: 0.42\n",
      "\n",
      "RV: x_56 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.23, min: 0.68\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.50, min: 0.52\n",
      "\n",
      "RV: x_57 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.25, min: 0.76\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.48, min: 0.52\n",
      "\n",
      "RV: x_58 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.25, min: 0.72\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.49, min: 0.53\n",
      "\n",
      "RV: x_59 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.23, min: 0.76\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.50, min: 0.56\n",
      "\n",
      "RV: x_60 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.27, min: 0.77\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.51, min: 0.54\n",
      "\n",
      "RV: x_61 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.23, min: 0.72\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.50, min: 0.51\n",
      "\n",
      "RV: x_62 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.25, min: 0.74\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.48, min: 0.55\n",
      "\n",
      "RV: x_63 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.26, min: 0.74\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.46, min: 0.52\n",
      "\n",
      "RV: x_64 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.29, min: 0.77\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.48, min: 0.54\n",
      "\n",
      "RV: x_65 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.23, min: 0.71\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.47, min: 0.54\n",
      "\n",
      "RV: x_66 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.24, min: 0.77\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.48, min: 0.51\n",
      "\n",
      "RV: x_67 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.27, min: 0.75\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.46, min: 0.54\n",
      "\n",
      "RV: x_68 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.29, min: 0.74\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.47, min: 0.50\n",
      "\n",
      "RV: x_69 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.23, min: 0.75\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.49, min: 0.55\n",
      "\n",
      "RV: x_70 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.29, min: 0.68\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.50, min: 0.54\n",
      "\n",
      "RV: x_71 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.24, min: 0.66\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.48, min: 0.53\n",
      "\n",
      "RV: x_72 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.24, min: 0.73\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.45, min: 0.51\n",
      "\n",
      "RV: x_73 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.30, min: 0.75\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.46, min: 0.53\n",
      "\n",
      "RV: x_74 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.31, min: 0.76\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.46, min: 0.51\n",
      "\n",
      "RV: x_75 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.26, min: 0.74\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.51, min: 0.51\n",
      "\n",
      "RV: x_76 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.11, max: 1.28, min: 0.65\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.44, min: 0.50\n",
      "\n",
      "RV: x_77 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.28, min: 0.77\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.50, min: 0.51\n",
      "\n",
      "RV: x_78 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.24, min: 0.73\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.48, min: 0.49\n",
      "\n",
      "RV: x_79 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.23, min: 0.75\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.43, min: 0.44\n",
      "\n",
      "RV: x_80 -- mean: 1.00, std: 0.10, distribution: normal\n",
      "LHS sample -- mean: 1.00, std: 0.10, max: 1.30, min: 0.77\n",
      "Monte Carlo sample -- mean: 1.00, std: 0.10, max: 1.48, min: 0.53\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_info(RVs, [SAMPLING_PLAN_STRATEGY, 'Monte Carlo'], [x, x_candidate])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate initial sampling plan, $\\Gamma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = limit_state_function(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4605,  5.3109,  5.5517,  3.9244,  7.5034,  5.6346,  5.8629,  6.1103,\n",
      "         4.9282,  4.2455,  5.3963,  7.0045,  6.4977,  4.1306,  5.2996,  6.6697,\n",
      "         5.6219,  4.8630,  7.0146,  5.9483,  4.3584,  6.9780,  3.0722,  5.4441,\n",
      "         1.5697,  3.7908,  3.9184,  6.1917,  6.2909,  6.6146,  3.7923,  4.5557,\n",
      "         8.6837,  5.3400,  4.7518,  9.6464,  6.2588,  2.8204,  6.0438,  3.6143,\n",
      "         4.1308,  3.6735,  5.8060,  3.6570,  4.8131,  8.3549,  3.8824,  5.4858,\n",
      "         3.3773,  5.0653,  6.5012,  4.5397,  7.1606,  2.7451,  5.2270,  5.9716,\n",
      "         4.8779,  8.1504,  3.2944,  6.9476,  3.8873,  6.8027,  6.6847, 10.2823,\n",
      "         6.1027,  8.9382,  6.9725,  6.7453,  4.8158,  3.9147,  8.1673,  3.3884,\n",
      "         7.4162,  7.9842,  6.2344,  1.9904,  6.1730,  6.1915,  3.2288,  5.5694,\n",
      "         4.6589])\n"
     ]
    }
   ],
   "source": [
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = x.shape[1]\n",
    "estimate_Pf_all = []\n",
    "estimate_Pf_allp = []\n",
    "estimate_Pf_allm = []\n",
    "estimate_N_samples_added = []\n",
    "N_samples_added_total = 0\n",
    "converged = False\n",
    "it = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 0\n",
      "Hyperparameters: [35, 15, 6, 2], SN: True, act_fun: Tanh\n",
      "Early stopping at epoch 150\n",
      "Best Loss: 1.984727919101715 at epoch 48. Training loss: 1.447919249534607 and val. loss: 2.5215365886688232\n",
      "  Training succeeded after 1 attempt(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\T-GAMER\\Desktop\\DOC\\Artigo SN\\AL-SNDGPR-MCS\\core\\SNDGPR\\train.py:103: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"best_model_and_likelihood.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. loss = 1.9847, delta = inf%, Pf: [1.8000000636675395e-05, 0.0, 0.0]\n",
      "Fold: 1, Avg. Loss: 1.984727919101715\n",
      "\n",
      "Early stopping at epoch 217\n",
      "Best Loss: 1.489379107952118 at epoch 115. Training loss: 1.3867563009262085 and val. loss: 1.5920019149780273\n",
      "  Training succeeded after 1 attempt(s).\n",
      "avg. loss = 1.4894, delta = inf%, Pf: [0.0009539999882690609, 0.0, 0.0]\n",
      "New best model found at fold 2:\n",
      "    delta = inf%, avg. loss = 1.4894\n",
      "    layer_sizes: [35, 15, 6, 2], act fun: <class 'torch.nn.modules.activation.Tanh'>\n",
      "\n",
      "Fold: 2, Avg. Loss: 1.489379107952118\n",
      "\n",
      "Early stopping at epoch 203\n",
      "Best Loss: 1.615769386291504 at epoch 101. Training loss: 1.443704605102539 and val. loss: 1.7878341674804688\n",
      "  Training succeeded after 1 attempt(s).\n",
      "avg. loss = 1.6158, delta = inf%, Pf: [0.00072900002123788, 0.0, 0.0]\n",
      "Fold: 3, Avg. Loss: 1.615769386291504\n",
      "\n",
      "Early stopping at epoch 231\n",
      "Best Loss: 1.3860222101211548 at epoch 129. Training loss: 1.2892876863479614 and val. loss: 1.4827567338943481\n",
      "  Training succeeded after 1 attempt(s).\n",
      "avg. loss = 1.3860, delta = inf%, Pf: [0.0017600000137463212, 0.0, 0.0]\n",
      "New best model found at fold 4:\n",
      "    delta = inf%, avg. loss = 1.3860\n",
      "    layer_sizes: [35, 15, 6, 2], act fun: <class 'torch.nn.modules.activation.Tanh'>\n",
      "\n",
      "Fold: 4, Avg. Loss: 1.3860222101211548\n",
      "\n",
      "Early stopping at epoch 326\n",
      "Best Loss: 1.7082326710224152 at epoch 224. Training loss: 0.946970522403717 and val. loss: 2.4694948196411133\n",
      "  Training succeeded after 1 attempt(s).\n",
      "avg. loss = 1.7082, delta = 185400.00%, Pf: [0.001854000031016767, 9.999999974752427e-07, 0.0]\n",
      "Fold: 5, Avg. Loss: 1.7082326710224152\n",
      "\n",
      "Early stopping at epoch 247\n",
      "Best Loss: 2.098972797393799 at epoch 145. Training loss: 1.1919705867767334 and val. loss: 3.0059750080108643\n",
      "  Training succeeded after 1 attempt(s).\n",
      "avg. loss = 2.0990, delta = inf%, Pf: [0.001756999990902841, 0.0, 0.0]\n",
      "Fold: 6, Avg. Loss: 2.098972797393799\n",
      "\n",
      "Average Loss across 6 folds: 1.7138506819804509\n",
      "\n",
      "obj fun (avg loss): 1.71 -> best fold: 4\n",
      "\n",
      "\n",
      "Hyperparameters: [23, 6, 2], SN: True, act_fun: Tanh\n",
      "Early stopping at epoch 138\n",
      "Best Loss: 2.084588944911957 at epoch 36. Training loss: 1.5393699407577515 and val. loss: 2.629807949066162\n",
      "  Training succeeded after 1 attempt(s).\n",
      "avg. loss = 2.0846, delta = inf%, Pf: [6.000000212225132e-06, 0.0, 0.0]\n",
      "Fold: 1, Avg. Loss: 2.084588944911957\n",
      "\n",
      "Early stopping at epoch 225\n",
      "Best Loss: 1.4106744527816772 at epoch 123. Training loss: 1.370540738105774 and val. loss: 1.4508081674575806\n",
      "  Training succeeded after 1 attempt(s).\n",
      "avg. loss = 1.4107, delta = inf%, Pf: [0.00145800004247576, 0.0, 0.0]\n",
      "New best model found at fold 2:\n",
      "    delta = inf%, avg. loss = 1.4107\n",
      "    layer_sizes: [23, 6, 2], act fun: <class 'torch.nn.modules.activation.Tanh'>\n",
      "\n",
      "Fold: 2, Avg. Loss: 1.4106744527816772\n",
      "\n",
      "Early stopping at epoch 196\n",
      "Best Loss: 1.5519809126853943 at epoch 94. Training loss: 1.496694564819336 and val. loss: 1.6072672605514526\n",
      "  Training succeeded after 1 attempt(s).\n",
      "avg. loss = 1.5520, delta = inf%, Pf: [0.0005080000264570117, 0.0, 0.0]\n",
      "Fold: 3, Avg. Loss: 1.5519809126853943\n",
      "\n",
      "Early stopping at epoch 153\n",
      "Best Loss: 1.9905844926834106 at epoch 51. Training loss: 1.536402940750122 and val. loss: 2.444766044616699\n",
      "  Training succeeded after 1 attempt(s).\n",
      "avg. loss = 1.9906, delta = inf%, Pf: [3.199999991920777e-05, 0.0, 0.0]\n",
      "Fold: 4, Avg. Loss: 1.9905844926834106\n",
      "\n",
      "Early stopping at epoch 422\n",
      "Best Loss: 1.6974357664585114 at epoch 320. Training loss: 0.6831560730934143 and val. loss: 2.7117154598236084\n",
      "  Training succeeded after 1 attempt(s).\n",
      "avg. loss = 1.6974, delta = inf%, Pf: [0.001154000055976212, 0.0, 0.0]\n",
      "Fold: 5, Avg. Loss: 1.6974357664585114\n",
      "\n",
      "Early stopping at epoch 117\n",
      "Best Loss: 2.0608826875686646 at epoch 15. Training loss: 1.9424140453338623 and val. loss: 2.179351329803467\n",
      "  Training succeeded after 1 attempt(s).\n",
      "avg. loss = 2.0609, delta = nan%, Pf: [0.0, 0.0, 0.0]\n",
      "Fold: 6, Avg. Loss: 2.0608826875686646\n",
      "\n",
      "Average Loss across 6 folds: 1.7993578761816025\n",
      "\n",
      "obj fun (avg loss): 1.80 -> best fold: 2\n",
      "\n",
      "\n",
      "Hyperparameters: [31, 12, 5, 2], SN: True, act_fun: ELU\n",
      "Early stopping at epoch 121\n",
      "Best Loss: 2.189658522605896 at epoch 19. Training loss: 1.686753273010254 and val. loss: 2.692563772201538\n",
      "  Training succeeded after 1 attempt(s).\n",
      "avg. loss = 2.1897, delta = inf%, Pf: [3.999999989900971e-06, 0.0, 0.0]\n",
      "Fold: 1, Avg. Loss: 2.189658522605896\n",
      "\n",
      "Early stopping at epoch 235\n",
      "Best Loss: 1.4067327380180359 at epoch 133. Training loss: 1.2983965873718262 and val. loss: 1.5150688886642456\n",
      "  Training succeeded after 1 attempt(s).\n",
      "avg. loss = 1.4067, delta = inf%, Pf: [0.0018289999570697546, 0.0, 0.0]\n",
      "New best model found at fold 2:\n",
      "    delta = inf%, avg. loss = 1.4067\n",
      "    layer_sizes: [31, 12, 5, 2], act fun: <class 'torch.nn.modules.activation.ELU'>\n",
      "\n",
      "Fold: 2, Avg. Loss: 1.4067327380180359\n",
      "\n",
      "Early stopping at epoch 215\n",
      "Best Loss: 1.2229297757148743 at epoch 113. Training loss: 1.3388175964355469 and val. loss: 1.1070419549942017\n",
      "  Training succeeded after 1 attempt(s).\n",
      "avg. loss = 1.2229, delta = inf%, Pf: [0.0007600000244565308, 0.0, 0.0]\n",
      "New best model found at fold 3:\n",
      "    delta = inf%, avg. loss = 1.2229\n",
      "    layer_sizes: [31, 12, 5, 2], act fun: <class 'torch.nn.modules.activation.ELU'>\n",
      "\n",
      "Fold: 3, Avg. Loss: 1.2229297757148743\n",
      "\n",
      "Early stopping at epoch 224\n",
      "Best Loss: 1.215748369693756 at epoch 122. Training loss: 1.288259744644165 and val. loss: 1.1432369947433472\n",
      "  Training succeeded after 1 attempt(s).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m it \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      5\u001b[0m     f_EGO, x_EGO, model, likelihood, train_losses, val_losses, train_x, val_x, train_g, val_g, x_max, x_min \\\n\u001b[1;32m----> 6\u001b[0m         \u001b[38;5;241m=\u001b[39m \u001b[43mbayesian_optimization\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_candidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_MC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mALPHA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_INITIAL_EGO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_INFILL_EGO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDIM_EGO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mTRAINING_ITERATIONS_EGO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBOUNDS_BSA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBSA_POPSIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBSA_EPOCH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mSEED\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTRAINING_ITERATIONS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBOUNDS_BAY_OPT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSPECTRAL_NORMALIZATION\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43mVALIDATION_SPLIT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mParams\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     layer_sizes, act_fun \u001b[38;5;241m=\u001b[39m optimization_variables(BOUNDS_BAY_OPT, x_EGO[torch\u001b[38;5;241m.\u001b[39margmin(f_EGO), :], x, SPECTRAL_NORMALIZATION)\n",
      "File \u001b[1;32mc:\\Users\\T-GAMER\\Desktop\\DOC\\Artigo SN\\AL-SNDGPR-MCS\\core\\bay_opt\\bayesian_optimization.py:28\u001b[0m, in \u001b[0;36mbayesian_optimization\u001b[1;34m(x, g, x_candidate, N, N_MC, ALPHA, N_INITIAL_EGO, N_INFILL_EGO, DIM_EGO, TRAINING_ITERATIONS_EGO, BOUNDS_BSA, BSA_POPSIZE, BSA_EPOCH, SEED, TRAINING_ITERATIONS, BOUNDS_BAY_OPT, SPECTRAL_NORMALIZATION, VALIDATION_SPLIT, LEARNING_RATE, Params)\u001b[0m\n\u001b[0;32m     23\u001b[0m x_EGO \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(lhs_sample)\n\u001b[0;32m     24\u001b[0m x_EGO \u001b[38;5;241m=\u001b[39m x_EGO\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     26\u001b[0m f_EGO, best_model, best_likelihood, best_train_losses, best_val_losses, \\\n\u001b[0;32m     27\u001b[0m       train_x, val_x, train_g, val_g, best_x_max, best_x_min \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m---> 28\u001b[0m     \u001b[43mobj_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_EGO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_candidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_MC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mALPHA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBOUNDS_BAY_OPT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSPECTRAL_NORMALIZATION\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVALIDATION_SPLIT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTRAINING_ITERATIONS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSEED\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mParams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m f_EGO \u001b[38;5;241m=\u001b[39m f_EGO\u001b[38;5;241m.\u001b[39mview(N_INITIAL_EGO)\n\u001b[0;32m     32\u001b[0m overall_best_model \u001b[38;5;241m=\u001b[39m best_model\n",
      "File \u001b[1;32mc:\\Users\\T-GAMER\\Desktop\\DOC\\Artigo SN\\AL-SNDGPR-MCS\\core\\bay_opt\\objective_function.py:29\u001b[0m, in \u001b[0;36mobj_fun\u001b[1;34m(xx, x, g, x_candidate, N, N_MC, ALPHA, BOUNDS_BAY_OPT, SPECTRAL_NORMALIZATION, VALIDATION_SPLIT, TRAINING_ITERATIONS, LEARNING_RATE, SEED, Params)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHyperparameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_sizes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, SN: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSPECTRAL_NORMALIZATION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, act_fun: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mact_fun\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Train the model with the sampled hyperparameters\u001b[39;00m\n\u001b[0;32m     28\u001b[0m fobj, loss, model, likelihood, train_losses, val_losses, train_x, val_x, train_g, val_g, x_max, x_min, fold \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m---> 29\u001b[0m     \u001b[43mkfold_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_candidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTRAINING_ITERATIONS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_MC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mALPHA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSPECTRAL_NORMALIZATION\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mParams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVALIDATION_SPLIT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSEED\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSEED\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobj fun (avg loss): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfobj\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> best fold: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     36\u001b[0m fobj_all[idx] \u001b[38;5;241m=\u001b[39m fobj\n",
      "File \u001b[1;32mc:\\Users\\T-GAMER\\Desktop\\DOC\\Artigo SN\\AL-SNDGPR-MCS\\core\\bay_opt\\K_fold_train.py:62\u001b[0m, in \u001b[0;36mkfold_train\u001b[1;34m(x, g, x_candidate, epochs, lr, layer_sizes, activation_fn, N, N_MC, ALPHA, SPECTRAL_NORMALIZATION, Params, n_splits, SEED)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Predict MC responses (only the sample which are not contained in the Kriging yet)\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mMC_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlikelihood\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_candidate_normalized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Evaluate learning function\u001b[39;00m\n\u001b[0;32m     65\u001b[0m g_mean, gs, _ \u001b[38;5;241m=\u001b[39m evaluate_lf(preds, learning_function)\n",
      "File \u001b[1;32mc:\\Users\\T-GAMER\\Desktop\\DOC\\Artigo SN\\AL-SNDGPR-MCS\\core\\MCS.py:95\u001b[0m, in \u001b[0;36mMC_prediction\u001b[1;34m(model, likelihood, x_candidate)\u001b[0m\n\u001b[0;32m     93\u001b[0m likelihood\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), use_toeplitz(\u001b[38;5;28;01mFalse\u001b[39;00m), fast_pred_var():\n\u001b[1;32m---> 95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_candidate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\T-GAMER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gpytorch\\models\\exact_gp.py:333\u001b[0m, in \u001b[0;36mExactGP.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;66;03m# Make the prediction\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mcg_tolerance(settings\u001b[38;5;241m.\u001b[39meval_cg_tolerance\u001b[38;5;241m.\u001b[39mvalue()):\n\u001b[0;32m    330\u001b[0m     (\n\u001b[0;32m    331\u001b[0m         predictive_mean,\n\u001b[0;32m    332\u001b[0m         predictive_covar,\n\u001b[1;32m--> 333\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexact_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_covar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;66;03m# Reshape predictive mean to match the appropriate event shape\u001b[39;00m\n\u001b[0;32m    336\u001b[0m predictive_mean \u001b[38;5;241m=\u001b[39m predictive_mean\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39mbatch_shape, \u001b[38;5;241m*\u001b[39mtest_shape)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[1;32mc:\\Users\\T-GAMER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gpytorch\\models\\exact_prediction_strategies.py:321\u001b[0m, in \u001b[0;36mDefaultPredictionStrategy.exact_prediction\u001b[1;34m(self, joint_mean, joint_covar)\u001b[0m\n\u001b[0;32m    317\u001b[0m     test_test_covar \u001b[38;5;241m=\u001b[39m joint_covar[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train :, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train :]\n\u001b[0;32m    318\u001b[0m     test_train_covar \u001b[38;5;241m=\u001b[39m joint_covar[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train :, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train]\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 321\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexact_predictive_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_train_covar\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexact_predictive_covar(test_test_covar, test_train_covar),\n\u001b[0;32m    323\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\T-GAMER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gpytorch\\models\\exact_prediction_strategies.py:346\u001b[0m, in \u001b[0;36mDefaultPredictionStrategy.exact_predictive_mean\u001b[1;34m(self, test_mean, test_train_covar)\u001b[0m\n\u001b[0;32m    344\u001b[0m nan_policy \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mobservation_nan_policy\u001b[38;5;241m.\u001b[39mvalue()\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nan_policy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 346\u001b[0m     res \u001b[38;5;241m=\u001b[39m (\u001b[43mtest_train_covar\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmean_cache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m nan_policy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;66;03m# Restrict train dimension to observed values\u001b[39;00m\n\u001b[0;32m    349\u001b[0m     observed \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mobservation_nan_policy\u001b[38;5;241m.\u001b[39m_get_observed(mean_cache, torch\u001b[38;5;241m.\u001b[39mSize((mean_cache\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],)))\n",
      "File \u001b[1;32mc:\\Users\\T-GAMER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\linear_operator\\operators\\_linear_operator.py:2905\u001b[0m, in \u001b[0;36mLinearOperator.__matmul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   2899\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__matmul__\u001b[39m(\n\u001b[0;32m   2900\u001b[0m     \u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   2901\u001b[0m     other: Union[\n\u001b[0;32m   2902\u001b[0m         Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch2 N D\u001b[39m\u001b[38;5;124m\"\u001b[39m], Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m\"\u001b[39m], Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch2 N D\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   2903\u001b[0m     ],\n\u001b[0;32m   2904\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... M D\u001b[39m\u001b[38;5;124m\"\u001b[39m], Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... M\u001b[39m\u001b[38;5;124m\"\u001b[39m], Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... M D\u001b[39m\u001b[38;5;124m\"\u001b[39m]]:\n\u001b[1;32m-> 2905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\T-GAMER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\linear_operator\\operators\\_linear_operator.py:1839\u001b[0m, in \u001b[0;36mLinearOperator.matmul\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   1835\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlinear_operator\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moperators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatmul_linear_operator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatmulLinearOperator\n\u001b[0;32m   1837\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m MatmulLinearOperator(\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m-> 1839\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Matmul\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentation_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, other, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresentation())\n",
      "File \u001b[1;32mc:\\Users\\T-GAMER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gpytorch\\lazy\\lazy_evaluated_kernel_tensor.py:406\u001b[0m, in \u001b[0;36mLazyEvaluatedKernelTensor.representation_tree\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrepresentation_tree()\n\u001b[0;32m    403\u001b[0m \u001b[38;5;66;03m# Otherwise, we'll evaluate the kernel (or at least its LinearOperator representation) and use its\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;66;03m# representation\u001b[39;00m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 406\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mrepresentation_tree()\n",
      "File \u001b[1;32mc:\\Users\\T-GAMER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gpytorch\\utils\\memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[1;32mc:\\Users\\T-GAMER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gpytorch\\lazy\\lazy_evaluated_kernel_tensor.py:25\u001b[0m, in \u001b[0;36mrecall_grad_state.<locals>.wrapped\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(method)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_grad_enabled):\n\u001b[1;32m---> 25\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\T-GAMER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gpytorch\\lazy\\lazy_evaluated_kernel_tensor.py:355\u001b[0m, in \u001b[0;36mLazyEvaluatedKernelTensor.evaluate_kernel\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    353\u001b[0m     temp_active_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mactive_dims\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mactive_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mactive_dims \u001b[38;5;241m=\u001b[39m temp_active_dims\n\u001b[0;32m    364\u001b[0m \u001b[38;5;66;03m# Check the size of the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\T-GAMER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gpytorch\\kernels\\kernel.py:530\u001b[0m, in \u001b[0;36mKernel.__call__\u001b[1;34m(self, x1, x2, diag, last_dim_is_batch, **params)\u001b[0m\n\u001b[0;32m    527\u001b[0m     res \u001b[38;5;241m=\u001b[39m LazyEvaluatedKernelTensor(x1_, x2_, kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, last_dim_is_batch\u001b[38;5;241m=\u001b[39mlast_dim_is_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     res \u001b[38;5;241m=\u001b[39m to_linear_operator(\n\u001b[1;32m--> 530\u001b[0m         \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mKernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx1_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m     )\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\T-GAMER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gpytorch\\module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[1;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[1;32mc:\\Users\\T-GAMER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gpytorch\\kernels\\scale_kernel.py:118\u001b[0m, in \u001b[0;36mScaleKernel.forward\u001b[1;34m(self, x1, x2, last_dim_is_batch, diag, **params)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m     outputscales \u001b[38;5;241m=\u001b[39m outputscales\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39moutputscales\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43morig_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputscales\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    print(f'\\nIteration {it}')\n",
    "    \n",
    "    if it == 0:\n",
    "        f_EGO, x_EGO, model, likelihood, train_losses, val_losses, train_x, val_x, train_g, val_g, x_max, x_min \\\n",
    "            = bayesian_optimization(\n",
    "                x, g, x_candidate, N, N_MC, ALPHA, N_INITIAL_EGO, N_INFILL_EGO, DIM_EGO, \\\n",
    "                TRAINING_ITERATIONS_EGO, BOUNDS_BSA, BSA_POPSIZE, BSA_EPOCH, \\\n",
    "                SEED, TRAINING_ITERATIONS, BOUNDS_BAY_OPT, SPECTRAL_NORMALIZATION, \\\n",
    "                VALIDATION_SPLIT, LEARNING_RATE, Params\n",
    "                )\n",
    "            \n",
    "    else:\n",
    "        layer_sizes, act_fun = optimization_variables(BOUNDS_BAY_OPT, x_EGO[torch.argmin(f_EGO), :], x, SPECTRAL_NORMALIZATION)\n",
    "        \n",
    "        _, _, model, likelihood, train_losses, val_losses, train_x, val_x, train_g, val_g, x_max, x_min, fold \\\n",
    "            = kfold_train(\n",
    "            x, g, x_candidate, TRAINING_ITERATIONS, LEARNING_RATE, layer_sizes, act_fun, \\\n",
    "                N, N_MC, ALPHA, SPECTRAL_NORMALIZATION, Params, n_splits=VALIDATION_SPLIT, SEED=SEED\n",
    "            )\n",
    "    \n",
    "    # Save variables and plot loss\n",
    "    save_bests(model, likelihood, train_losses, val_losses, x, train_x, val_x, train_g, val_g,\n",
    "               x_EGO, f_EGO, x_max, x_min, it, BOUNDS_BAY_OPT, SPECTRAL_NORMALIZATION, EXAMPLE)\n",
    "    plot_losses(train_losses, val_losses, it)\n",
    "    \n",
    "    # Predict MC responses (only the sample which are not contained in the Kriging yet)\n",
    "    x_candidate_normalized = min_max_normalization(x_max, x_min, x_candidate)\n",
    "    preds = MC_prediction(model, likelihood, x_candidate_normalized)\n",
    "    \n",
    "    # Evaluate learning function\n",
    "    g_mean, gs, ind_lf = evaluate_lf(preds, learning_function)\n",
    "    \n",
    "    # Select additional sample (the sample which maximizes the learning function value)\n",
    "    x_added = x_candidate[ind_lf, :]\n",
    "    x_added = x_added.view(1, data_dim)\n",
    "    # x_added = x_added * (x_max - x_min) + x_min  # undo normalization\n",
    "    \n",
    "    save_x_added(x_added, it, EXAMPLE)  # Salve o array do NumPy em um arquivo .mat\n",
    "    \n",
    "    # Estimate Pf\n",
    "    Pf, Pf_plus, Pf_minus = estimate_Pf(g, g_mean, gs, N, N_MC, ALPHA)\n",
    "    \n",
    "    estimate_Pf_all.append(Pf)\n",
    "    estimate_Pf_allp.append(Pf_plus)\n",
    "    estimate_Pf_allm.append(Pf_minus)\n",
    "    estimate_N_samples_added.append(N_samples_added_total)\n",
    "    \n",
    "    # Print some info\n",
    "    print_info(N, N_INFILL, it, Pf, Pf_plus, Pf_minus)\n",
    "    \n",
    "    # Check if maximum number of points were added\n",
    "    if N_samples_added_total >= N_INFILL: break\n",
    "    it += 1\n",
    "    \n",
    "    # Convergence criterion\n",
    "    if converged and N_samples_added_total != 0:\n",
    "        if convergence_function(g, g_mean, gs, N, N_MC): break\n",
    "        converged = False\n",
    "    else:\n",
    "        converged = convergence_function(g, g_mean, gs, N, N_MC)\n",
    "    \n",
    "    g_added = evaluate_g(x_added, it, limit_state_function, EXAMPLE)\n",
    "    \n",
    "    x = torch.cat((x, x_added), 0)\n",
    "    g = torch.cat((g, g_added), 0)\n",
    "    x_candidate = torch.cat((x_candidate[:ind_lf], x_candidate[ind_lf+1:]))\n",
    "    N_samples_added_total = N_samples_added_total + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "# Estimate failure probability\n",
    "estimate_Pf_0 = (torch.sum(g_mean <= 0) + torch.sum(g[N+1:] <= 0))/N_MC\n",
    "\n",
    "# Estimate the covariance\n",
    "estimate_CoV = torch.sqrt((1-estimate_Pf_0) / estimate_Pf_0 / N_MC)\n",
    "\n",
    "# Store the results\n",
    "Results = {\n",
    "    'Pf': estimate_Pf_0,\n",
    "    'Beta': -ndtri(estimate_Pf_0),\n",
    "    'CoV': estimate_CoV,\n",
    "    'Model_Evaluations': N_samples_added_total + N,\n",
    "    'Pf_CI': estimate_Pf_0 * np.array([\n",
    "        1 + ndtri(ALPHA/2)*estimate_CoV,\n",
    "        1 + ndtri(1-ALPHA/2)*estimate_CoV\n",
    "        ]),\n",
    "    }\n",
    "Results['Beta_CI'] = torch.flip(-ndtri(Results['Pf_CI']), [0])\n",
    "\n",
    "History = {\n",
    "    'Pf': estimate_Pf_all,\n",
    "    'Pf_Upper': estimate_Pf_allp,\n",
    "    'Pf_Lower': estimate_Pf_allm,\n",
    "    'N_Samples': estimate_N_samples_added,\n",
    "    'N_Init': N,\n",
    "    'X': x,\n",
    "    'G': g,\n",
    "    'MC_Sample': x_candidate,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_save(Results, History, Params, EXAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display results and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_print(Results, History, Params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_plot(Results, History, Params, EXAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results, History, Params = pickle_load(EXAMPLE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
